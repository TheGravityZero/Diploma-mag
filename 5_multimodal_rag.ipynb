{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","root_path = \"/content/drive/MyDrive/Diploma-mag\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x45VDmDa5c65","executionInfo":{"status":"ok","timestamp":1748385718955,"user_tz":-180,"elapsed":35608,"user":{"displayName":"Руслан","userId":"08306979938033836096"}},"outputId":"ec6a24f4-b98b-4443-f43a-63086780c15a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDE3M3e84J7-"},"outputs":[],"source":["import os\n","from dotenv import load_dotenv, find_dotenv\n","\n","from langchain_gigachat import Gigachat\n","from langchain_gigachat.embeddings import GigaChatEmbeddings\n","\n","\n","load_dotenv(find_dotenv())\n","\n","def init_gigachat():\n","    return Gigachat(credentials=\"ключ_авторизации\", model=\"GigaChat-Max\", verify_ssl_certs=False, temperature=1e-15, timeout=100)\n","\n","\n","def init_gigachat_embeddings():\n","    return GigaChatEmbeddings(credentials=\"ключ_авторизации\", scope-\"GIGACHAT_API_PER\", verify_ssl_certs=False)"]},{"cell_type":"code","source":["import torch\n","import open_clip\n","\n","from PIL import Image\n","from typing import List, Union\n","from langchain_core.embeddings import Embeddings\n","from tqdm import tqdm\n","\n","\n","class OpenCLIPEmbeddings(Embeddings):\n","    def __init__(self, model_name: str = \"ViT-H-14\", pretrained: str = \"/Users/21451364/Downloads/CLIP-ViT-H-14-laion2B-s32B-b79K/open_clip_model.safetensors\"):\n","        self.model,  self.preprocess = open_clip.create_model_from_pretrained(model_name, pretrained=pretrained)\n","        self.tokenizer = open_clip.get_tokenizer(model_name)\n","        self.model.eval()\n","\n","    def _normalize(self, tensor: torch.Tensor) -> List[float]:\n","        return tensor.div(tensor.norm(p=2, dim=-1, keepdim=True)).squeeze(0).tolist()\n","\n","    def embed_text(self, text: str) -> List[float]:\n","        tokens = self.tokenizer(text)\n","        with torch.no_grad():\n","            embeddings = self.model.encode_text(tokens)\n","        return self._normalize(embeddings)\n","\n","    def embed_image(self, uris: List[str]) -> List[List[float]]:\n","        pil_images = [Image.open(uri) for uri in uris]\n","        image_features = []\n","        for pil_image in tqdm(pil_images):\n","            preprocessed_image = self.preprocess(pil_image).unsqueeze(0)\n","            embeddings_tensor = self.model.encode_image(preprocessed_image)\n","            norm = embeddings_tensor.norm(p=2, dim=1, keepdim=True)\n","            normalized_embeddings_tensor = embeddings_tensor.div(norm)\n","            embeddings_list = normalized_embeddings_tensor.squeeze(0).tolist()\n","            image_features.append(embeddings_list)\n","        return image_features\n","\n","    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n","        return [self.embed_image(text) for text in texts]\n","\n","    def embed_query(self, text: str) -> List[float]:\n","        return self.embed_text(text)"],"metadata":{"id":"JAhPjuoaPNZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NqG3jP6h4J8E"},"outputs":[],"source":["llm = init_gigachat()\n","embeddings = init_gigachat_embeddings()\n","image_embeddigns = OpenCLIPEmbeddings()\n","\n","model_id = \"gigachat\"\n","text_embedding_model = \"gigaEmbeddings\"\n","image_embeddign_model = \"clip\"\n","\n","img_vectorstore_dir = f\"./data/multimodal_rag/image_{model_id}_vectorstore_{image_embeddign_model}\"\n","text_vectorstore_dir = f\"./data/multimodal_rag/text_{model_id}_vectorstore_{text_embedding_model}\""]},{"cell_type":"code","source":["from langchain_chroma import Chroma\n","from chromadb.config import Settings\n","\n","text_vectorstore = Chroma(\n","    persist_directory=text_vectorstore_dir,\n","    embedding_function=embeddings,\n","    collection_name=\"mm_rag_text_gigaembeddings\",\n","    client_settings=Settings(anonymized_telemetry=False)\n",")\n","\n","img_vectorstore = Chroma(\n","        persist_directory=img_vectorstore_dir,\n","        embedding_function=image_embeddigns,\n","        collection_name=f\"mm_rag_image_gigaembeddings\",\n","        client_settings=Settings(anonymized_telemetry=False)\n",")"],"metadata":{"id":"4TppRLi6PT_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","with open(\"./extracted_data/extracted_texts.json\", \"r\") as f:\n","    documents = json.load(f)\n","\n","with open(\"./extracted_data/extracted_images.json\", \"r\") as f:\n","    images = json.load(f)"],"metadata":{"id":"EttfQgq2eQMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["uris = [f\"./extracted_data/source_images/image_{j}.png\" for j in range(len(images))]\n","metadatas = [{\"page_number\": images[j][\"page_number\"], \"image_path\": uris[j]} for j in range(len(images))]\n","\n","img_vectorstore.add_images(\n","    uris=uris,\n","    metadatas=metadatas\n",")"],"metadata":{"id":"kg_WOX-FPYbo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","with open(\"texts.json\", \"r\") as f:\n","    documents = json.load(f)\n","\n","with open(\"images.json\", \"r\") as f:\n","    images = json.load(f)"],"metadata":{"id":"eoiR7G2vPUcY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import uuid\n","from langchain_core.documents import Document\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n","\n","documents_content = []\n","documents_page = []\n","\n","for d in documents:\n","    documents_content.append(d[\"text\"])\n","    documents_page.append(d[\"metadata\"][\"page_number\"])\n","\n","prepared_text = []\n","for i, document_content in enumerate(documents_content):\n","    chunks = text_splitter.split_text(document_content)\n","    doc_ids += [str(uuid.uuid4()) for _ in chunks]\n","    prepared_text += [Document(\n","        page_content=chunk,\n","        metadata={\n","            \"page_number\": documents_page[i]\n","        })\n","    for j, chunk in enumerate(chunks)]\n","\n","all_chunks = [text.page_content for text in prepared_text]\n","\n","text_vectorstore.add_documents(prepared_text)"],"metadata":{"id":"a5NqwG76PcvY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage\n","from langchain.prompts import ChatPromptTemplate\n","from prompts import QA_PROMPT_SYSTEM, QA_PROMPT_USER\n","\n","def run_pipeline(question, text_vectorstore, img_vectorstore, llm):\n","    text_content = text_vectorstore.similarity_search(question, k=4)\n","    image_content = img_vectorstore.similarity_search(question, k=1)\n","\n","    context = \"\\n\\n\".join([t.page_content for t in text_content])\n","    img_path = image_content[0].metadata[\"image_path\"]\n","    file = llm.upload_file(open(img_path, \"rb\"))\n","\n","    text_content = QA_PROMPT_USER.format(context=context, question=question)\n","\n","    prompt = ChatPromptTemplate.from_messages(\n","        [\n","            (\"system\", QA_PROMPT_SYSTEM),\n","            HumanMessage(content=text_content, additional_kwargs={\"attachments\": [file.id_]})\n","        ]\n","    )\n","    chain = prompt | llm\n","\n","    return  chain.invoke({}).content"],"metadata":{"id":"ADJYK0FsPjCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = \"Как выделить прямоугольную область на изображении в Adobe Photoshop?\"\n","run_pipeline(question, text_vectorstore, img_vectorstore, llm)"],"metadata":{"id":"oDTfoCwIPlFO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LLaVa"],"metadata":{"id":"x6khfAQR4RoT"}},{"cell_type":"code","source":["!pip install -q -U transformers bitsandbytes accelerate\n","#!pip install -q bitsandbytes==0.41.3 accelerate==0.25.0"],"metadata":{"id":"N3FgMdw-4Sv9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BitsAndBytesConfig, LlavaNextProcessor, LlavaNextForConditionalGeneration\n","from PIL import Image\n","import io\n","import pandas as pd\n","from typing import Tuple\n","\n","quantization_config = BitsAndBytesConfig(\n","    load_in_8bit=True,\n","    low_cpu_mem_usage=True,\n","    use_flash_attention_2=True\n",")\n","\n","def format_prompt_with_image(prompt: str) -> str:\n","    return f\"[INST] <image>\\n{prompt} [/INST]\"\n","\n","\n","def get_qa_prompt(model_id:str, system_prompt:str, question: str, context: str, image: Image=None) -> str:\n","    if \"vicuna\" in model_id:\n","        prompt = f\"USER:{'<image>' if image else ' '}\\n{system_prompt}\\n{context}\\n\\nQuestion:\\n{question}\\n\\nASSISTANT:\"\n","    else:   # mistral\n","        prompt = f\"[INST]{'<image>' if image else ' '}\\n{system_prompt}\\n{context}\\n\\nQuestion:\\n{question}\\n\\n[/INST]\"\n","    return prompt\n","\n","\n","def format_output(raw_output, processor: LlavaNextProcessor, prompt: str) -> str:\n","    out = processor.decode(raw_output[0], skip_special_tokens=True)\n","    out_prompt = prompt.replace(\"<image>\", \" \").strip()\n","    formatted_output = out.replace(out_prompt, \"\").strip()\n","    return formatted_output\n","\n","\n","def get_prompt(task: str, model_id: str, system_prompt: str, text: str, image: Image, question: str) -> str:\n","    prompt = get_qa_prompt(model_id, system_prompt, question, text, image)\n","    return prompt\n","\n","\n","def llava_call(prompt: str, model: LlavaNextForConditionalGeneration, processor: LlavaNextProcessor, device: str, image: Image=None) -> str:\n","    inputs = processor(prompt, image, return_tensors=\"pt\").to(device)\n","    raw_output = model.generate(**inputs, max_new_tokens=300)\n","    formatted_output = format_output(raw_output, processor, prompt)\n","    return formatted_output\n","\n","\n","def load_llava_model(model_id: str) -> Tuple[LlavaNextForConditionalGeneration, LlavaNextProcessor]:\n","    processor = LlavaNextProcessor.from_pretrained(model_id)\n","    model = LlavaNextForConditionalGeneration.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\", cache_dir=root_path + \"/models\")\n","    #model = LlavaNextForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n","\n","    return model, processor"],"metadata":{"id":"MUFxZPIhZ2N-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from prompts import QA_PROMPT_SYSTEM, QA_PROMPT_USER\n","\n","def run_pileline_llava(question, text_vectorstore, img_vectorstore, model, processor, device=\"cuda\"):\n","    text_content = text_vectorstore.similarity_search(question, k=4)\n","    context = \"\\n\\n\".join([t.page_content for t in text_content])\n","    image_content = img_vectorstore.similarity_search(question, k=1)\n","\n","    if len(image_content) > 0:\n","      img_path = image_content[0].metadata[\"image_path\"]\n","      image = Image.open(img_path)\n","      img_prompt = get_qa_prompt(\"llava-hf/llava-v1.6-mistral-7b-hf\", QA_PROMPT_SYSTEM, question, context, image)\n","      return llava_call(img_prompt, model, processor, device, image)\n","\n","    no_img_prompt = get_qa_prompt(\"llava-hf/llava-v1.6-mistral-7b-hf\", QA_PROMPT_SYSTEM, question, context)\n","    return llava_call(no_img_prompt, model, processor, device)\n","\n",""],"metadata":{"id":"A95OjdWEbkpd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = \"What are the possible positions of the manual operator and what colors are associated with each position?\"\n","model, processor = load_llava_model(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n","model = model.eval()\n","\n","run_pileline_llava(question, text_vectorstore, img_vectorstore, model, processor)"],"metadata":{"id":"0opZmJEAZ9CK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Оценка ответа"],"metadata":{"id":"Q2DRZ2gtjtX_"}},{"cell_type":"markdown","source":[],"metadata":{"id":"xZiFmM5Wjr4m"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.11"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}