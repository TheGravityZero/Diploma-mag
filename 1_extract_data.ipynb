{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Извлечение данных с PDF файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linux\n",
    "# !apt-get install poppler-utils tesseract-ocr libmagic-dev\n",
    "\n",
    "# for mac\n",
    "# !brew install poppler tesseract libmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -Uq \"unstructured[all-docs]\" pillow lxml pillow\n",
    "#%pip install -Uq chromadb tiktoken\n",
    "#%pip install -Uq langchain langchain-community langchain-openai langchain-groq\n",
    "#%pip install -Uq python_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "PATH_TO_PDF_FILE = \"./photoshop_manual.pdf\"\n",
    "\n",
    "# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    infer_table_structure=True,            # extract tables\n",
    "    strategy=\"hi_res\",                     # mandatory to infer tables\n",
    "    extract_image_block_types=[\"Image\"],  \n",
    "    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
    "    chunking_strategy=\"by_title\",          # or 'basic'\n",
    "    max_characters=10000,                  # defaults to 500\n",
    "    combine_text_under_n_chars=2000,       # defaults to 0\n",
    "    new_after_n_chars=6000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables(chunks):\n",
    "    tables = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Table\" in str(type(el)):\n",
    "                    tables.append({\"image_base64\": el.metadata.image_base64, \"page_number\": el.metadata.page_number})\n",
    "    return tables\n",
    "\n",
    "tables = get_tables(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the images from the CompositeElement objects\n",
    "def get_images_base64(chunks):\n",
    "    images_b64 = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Image\" in str(type(el)):\n",
    "                    images_b64.append(el.metadata.image_base64)\n",
    "    return images_b64\n",
    "\n",
    "images = get_images_base64(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "    # Decode the base64 string to binary\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    # Display the image\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "display_base64_image(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./extracted_data/extracted_images.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(images, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"./extracted_data/extracted_texts.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(texts, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"./extracted_data/extracted_tables.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(texts, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "#with open(\"./extracted_data/extracted_images.json\", \"r\") as file:\n",
    "#    images = json.load(file)\n",
    "\n",
    "#with open(\"./extracted_data/extracted_texts.json\", \"r\") as file:\n",
    "#    texts = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import base64\n",
    "\n",
    "# Сохраняем кизображения в формате png для обраюотки через gigachat\n",
    "#for i in range(len(images)):\n",
    "#    image_data = base64.b64decode(images[i][\"image_base64\"])\n",
    "#    with open(f\"./extracted_data/source_images/image_{i}.png\", \"wb\") as file:\n",
    "#        file.write(image_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Саммари по картинкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_gigacaht import Gigachat\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "def init_gigachat():\n",
    "    return Gigachat(model=\"GigaChat-Max\", verify_ssl_certs=False, temperature=1e-15, timeout=100)\n",
    "\n",
    "llm = init_gigachat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from prompts import IMAGE_SUMMARY_PROMPT\n",
    "\n",
    "llm = init_gigachat()\n",
    "\n",
    "def get_gigachat_image_summary(prompt, file_path, llm):\n",
    "    file = llm.upload_file(open(file_path, \"rb\"))\n",
    "    image_summary = llm.invoke([HumanMessage(\n",
    "        content=IMAGE_SUMMARY_PROMPT,\n",
    "        additional_kwargs={\"attachemant\": [file.id_]}\n",
    "    )]).content\n",
    "    return image_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagies_summary = []\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while i < len(images):\n",
    "    image_summary = get_gigachat_image_summary(prompt, \"./extracted_data/source_images/image_{i}.png\", llm)\n",
    "    imagies_summary.append({\"image_summary\": image_summary, \"source\": \"./extracted_data/source_images/image_{i}.png\", \"page_number\": images[i][\"page_number\"]})\n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i}/{len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./extracted_data/imagies_summary.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(texts, file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
